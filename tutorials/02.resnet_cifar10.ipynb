{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial 2. Employing OTO onto ResNet18 CIFAR10. \n",
    "\n",
    "\n",
    "In this tutorial, we will show \n",
    "\n",
    "- How to end-to-end train and compress a ResNet18 from scratch on CIFAR10 to get a compressed ResNet18.\n",
    "- The compressed ResNet18 achives both **high performance** and **significant FLOPs and parameters reductions** than the full model. \n",
    "- More detailed DHSPG optimizer setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Create OTO instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from backends import resnet18_cifar10\n",
    "from only_train_once import OTO\n",
    "\n",
    "model = resnet18_cifar10()\n",
    "dummy_input = torch.zeros(1, 3, 32, 32)\n",
    "oto = OTO(model=model.cuda(), dummy_input=dummy_input.cuda())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Visualize the dependancy graph of DNN for ZIG partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A ResNet_zig.gv.pdf will be generated to display the depandancy graph.\n",
    "oto.visualize_zigs()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "trainset = CIFAR10(root='cifar10', train=True, download=True, transform=transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32, 4),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "testset = CIFAR10(root='cifar10', train=False, download=True, transform=transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]))\n",
    "\n",
    "trainloader =  torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=4)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False, num_workers=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Setup DHSPG optimizer\n",
    "\n",
    "The following main hyperparameters need to be taken care.\n",
    "\n",
    "- `variant`: The optimizer that is used for training the baseline full model. \n",
    "- `lr`: The initial learning rate.\n",
    "- `weight_decay`: Weight decay as standard DNN optimization.\n",
    "- `target_group_sparsity`: The target group sparsity, typically higher group sparsity refers to more FLOPs and model size reduction, meanwhile may regress model performance more.\n",
    "- `start_pruning_steps`: The number of steps that start to prune. \n",
    "- `epsilon`: The cofficient [0, 1) to control the aggresiveness of group sparsity exploration. Higher value means more aggressive group sparsity exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = oto.dhspg(\n",
    "    variant='sgd', \n",
    "    lr=0.1, \n",
    "    target_group_sparsity=0.7,\n",
    "    warm_up_steps=50,\n",
    "    weight_decay=1e-4,\n",
    "    start_pruning_steps=50 * len(trainloader), # start pruning after 50 epochs\n",
    "    epsilon=0.95)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Train ResNet18 as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tianyi/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 1.67, omega:4122.27, group_sparsity: 0.00, acc1: 0.3347\n",
      "Epoch: 1, loss: 1.15, omega:4112.58, group_sparsity: 0.00, acc1: 0.4246\n",
      "Epoch: 2, loss: 0.88, omega:4103.96, group_sparsity: 0.00, acc1: 0.5233\n",
      "Epoch: 3, loss: 0.71, omega:4094.89, group_sparsity: 0.00, acc1: 0.6019\n",
      "Epoch: 4, loss: 0.60, omega:4084.36, group_sparsity: 0.00, acc1: 0.7093\n",
      "Epoch: 5, loss: 0.52, omega:4073.36, group_sparsity: 0.00, acc1: 0.7460\n",
      "Epoch: 6, loss: 0.47, omega:4062.06, group_sparsity: 0.00, acc1: 0.7607\n",
      "Epoch: 7, loss: 0.43, omega:4050.87, group_sparsity: 0.00, acc1: 0.7199\n",
      "Epoch: 8, loss: 0.39, omega:4039.35, group_sparsity: 0.00, acc1: 0.7358\n",
      "Epoch: 9, loss: 0.36, omega:4028.00, group_sparsity: 0.00, acc1: 0.4426\n",
      "Epoch: 10, loss: 0.33, omega:4015.55, group_sparsity: 0.00, acc1: 0.8550\n",
      "Epoch: 11, loss: 0.30, omega:4003.24, group_sparsity: 0.00, acc1: 0.8667\n",
      "Epoch: 12, loss: 0.28, omega:3990.84, group_sparsity: 0.00, acc1: 0.8558\n",
      "Epoch: 13, loss: 0.27, omega:3978.91, group_sparsity: 0.00, acc1: 0.7943\n",
      "Epoch: 14, loss: 0.25, omega:3966.56, group_sparsity: 0.00, acc1: 0.8208\n",
      "Epoch: 15, loss: 0.23, omega:3954.08, group_sparsity: 0.00, acc1: 0.7998\n",
      "Epoch: 16, loss: 0.22, omega:3941.56, group_sparsity: 0.00, acc1: 0.8204\n",
      "Epoch: 17, loss: 0.21, omega:3929.43, group_sparsity: 0.00, acc1: 0.8268\n",
      "Epoch: 18, loss: 0.20, omega:3917.32, group_sparsity: 0.00, acc1: 0.8257\n",
      "Epoch: 19, loss: 0.18, omega:3904.39, group_sparsity: 0.00, acc1: 0.8419\n",
      "Epoch: 20, loss: 0.18, omega:3892.04, group_sparsity: 0.00, acc1: 0.8462\n",
      "Epoch: 21, loss: 0.17, omega:3879.71, group_sparsity: 0.00, acc1: 0.7912\n",
      "Epoch: 22, loss: 0.16, omega:3867.21, group_sparsity: 0.00, acc1: 0.8559\n",
      "Epoch: 23, loss: 0.15, omega:3854.67, group_sparsity: 0.00, acc1: 0.8890\n",
      "Epoch: 24, loss: 0.14, omega:3841.95, group_sparsity: 0.00, acc1: 0.7917\n",
      "Epoch: 25, loss: 0.13, omega:3829.31, group_sparsity: 0.00, acc1: 0.8416\n",
      "Epoch: 26, loss: 0.13, omega:3816.94, group_sparsity: 0.00, acc1: 0.8169\n",
      "Epoch: 27, loss: 0.12, omega:3804.81, group_sparsity: 0.00, acc1: 0.8054\n",
      "Epoch: 28, loss: 0.11, omega:3791.35, group_sparsity: 0.00, acc1: 0.8961\n",
      "Epoch: 29, loss: 0.11, omega:3778.11, group_sparsity: 0.00, acc1: 0.9008\n",
      "Epoch: 30, loss: 0.11, omega:3765.33, group_sparsity: 0.00, acc1: 0.9066\n",
      "Epoch: 31, loss: 0.10, omega:3752.16, group_sparsity: 0.00, acc1: 0.9082\n",
      "Epoch: 32, loss: 0.09, omega:3739.01, group_sparsity: 0.00, acc1: 0.8032\n",
      "Epoch: 33, loss: 0.09, omega:3725.78, group_sparsity: 0.00, acc1: 0.8897\n",
      "Epoch: 34, loss: 0.09, omega:3712.42, group_sparsity: 0.00, acc1: 0.8946\n",
      "Epoch: 35, loss: 0.08, omega:3698.36, group_sparsity: 0.00, acc1: 0.8953\n",
      "Epoch: 36, loss: 0.08, omega:3685.77, group_sparsity: 0.00, acc1: 0.8988\n",
      "Epoch: 37, loss: 0.08, omega:3672.62, group_sparsity: 0.00, acc1: 0.9082\n",
      "Epoch: 38, loss: 0.07, omega:3658.29, group_sparsity: 0.00, acc1: 0.9117\n",
      "Epoch: 39, loss: 0.07, omega:3644.33, group_sparsity: 0.00, acc1: 0.9186\n",
      "Epoch: 40, loss: 0.07, omega:3630.58, group_sparsity: 0.00, acc1: 0.9108\n",
      "Epoch: 41, loss: 0.06, omega:3616.41, group_sparsity: 0.00, acc1: 0.9028\n",
      "Epoch: 42, loss: 0.06, omega:3602.39, group_sparsity: 0.00, acc1: 0.9131\n",
      "Epoch: 43, loss: 0.06, omega:3588.52, group_sparsity: 0.00, acc1: 0.8836\n",
      "Epoch: 44, loss: 0.06, omega:3575.28, group_sparsity: 0.00, acc1: 0.8941\n",
      "Epoch: 45, loss: 0.06, omega:3561.09, group_sparsity: 0.00, acc1: 0.9111\n",
      "Epoch: 46, loss: 0.05, omega:3547.39, group_sparsity: 0.00, acc1: 0.8869\n",
      "Epoch: 47, loss: 0.05, omega:3533.47, group_sparsity: 0.00, acc1: 0.8086\n",
      "Epoch: 48, loss: 0.05, omega:3519.63, group_sparsity: 0.00, acc1: 0.8629\n",
      "Epoch: 49, loss: 0.05, omega:3504.83, group_sparsity: 0.00, acc1: 0.8159\n",
      "39100 39100\n",
      "partition_groups\n",
      "Epoch: 50, loss: 0.05, omega:3291.37, group_sparsity: 0.00, acc1: 0.9166\n",
      "Epoch: 51, loss: 0.06, omega:3077.04, group_sparsity: 0.00, acc1: 0.9093\n",
      "Epoch: 52, loss: 0.07, omega:2859.65, group_sparsity: 0.00, acc1: 0.8642\n",
      "Epoch: 53, loss: 0.09, omega:2640.33, group_sparsity: 0.00, acc1: 0.8693\n",
      "Epoch: 54, loss: 0.12, omega:2419.49, group_sparsity: 0.00, acc1: 0.8626\n",
      "Epoch: 55, loss: 0.15, omega:2200.25, group_sparsity: 0.00, acc1: 0.8525\n",
      "Epoch: 56, loss: 0.19, omega:1987.25, group_sparsity: 0.00, acc1: 0.7138\n",
      "Epoch: 57, loss: 0.23, omega:1786.97, group_sparsity: 0.02, acc1: 0.7841\n",
      "Epoch: 58, loss: 0.25, omega:1604.73, group_sparsity: 0.05, acc1: 0.8215\n",
      "Epoch: 59, loss: 0.28, omega:1443.93, group_sparsity: 0.11, acc1: 0.8154\n",
      "Epoch: 60, loss: 0.29, omega:1312.36, group_sparsity: 0.26, acc1: 0.7854\n",
      "Epoch: 61, loss: 0.30, omega:1218.12, group_sparsity: 0.42, acc1: 0.7661\n",
      "Epoch: 62, loss: 0.31, omega:1144.11, group_sparsity: 0.47, acc1: 0.7801\n",
      "Epoch: 63, loss: 0.33, omega:1086.31, group_sparsity: 0.52, acc1: 0.8057\n",
      "Epoch: 64, loss: 0.34, omega:1055.69, group_sparsity: 0.58, acc1: 0.6295\n",
      "Epoch: 65, loss: 0.35, omega:1043.84, group_sparsity: 0.65, acc1: 0.7786\n",
      "Epoch: 66, loss: 0.34, omega:1045.31, group_sparsity: 0.68, acc1: 0.8019\n",
      "Epoch: 67, loss: 0.32, omega:1051.97, group_sparsity: 0.70, acc1: 0.7147\n",
      "Epoch: 68, loss: 0.29, omega:1078.08, group_sparsity: 0.70, acc1: 0.8448\n",
      "Epoch: 69, loss: 0.26, omega:1097.87, group_sparsity: 0.70, acc1: 0.8552\n",
      "Epoch: 70, loss: 0.24, omega:1113.81, group_sparsity: 0.70, acc1: 0.8441\n",
      "Epoch: 71, loss: 0.22, omega:1128.12, group_sparsity: 0.70, acc1: 0.8564\n",
      "Epoch: 72, loss: 0.21, omega:1140.91, group_sparsity: 0.70, acc1: 0.8653\n",
      "Epoch: 73, loss: 0.20, omega:1152.43, group_sparsity: 0.70, acc1: 0.7971\n",
      "Epoch: 74, loss: 0.14, omega:1151.87, group_sparsity: 0.70, acc1: 0.9194\n",
      "Epoch: 75, loss: 0.11, omega:1151.28, group_sparsity: 0.70, acc1: 0.9196\n",
      "Epoch: 76, loss: 0.10, omega:1150.68, group_sparsity: 0.70, acc1: 0.9228\n",
      "Epoch: 77, loss: 0.09, omega:1150.06, group_sparsity: 0.70, acc1: 0.9229\n",
      "Epoch: 78, loss: 0.09, omega:1149.46, group_sparsity: 0.70, acc1: 0.9219\n",
      "Epoch: 79, loss: 0.08, omega:1148.86, group_sparsity: 0.70, acc1: 0.9216\n",
      "Epoch: 80, loss: 0.08, omega:1148.25, group_sparsity: 0.70, acc1: 0.9236\n",
      "Epoch: 81, loss: 0.08, omega:1147.64, group_sparsity: 0.70, acc1: 0.9230\n",
      "Epoch: 82, loss: 0.07, omega:1147.02, group_sparsity: 0.70, acc1: 0.9238\n",
      "Epoch: 83, loss: 0.07, omega:1146.39, group_sparsity: 0.70, acc1: 0.9224\n",
      "Epoch: 84, loss: 0.07, omega:1145.77, group_sparsity: 0.70, acc1: 0.9243\n",
      "Epoch: 85, loss: 0.06, omega:1145.15, group_sparsity: 0.70, acc1: 0.9249\n",
      "Epoch: 86, loss: 0.06, omega:1144.54, group_sparsity: 0.70, acc1: 0.9243\n",
      "Epoch: 87, loss: 0.06, omega:1143.93, group_sparsity: 0.70, acc1: 0.9250\n",
      "Epoch: 88, loss: 0.06, omega:1143.32, group_sparsity: 0.70, acc1: 0.9246\n",
      "Epoch: 89, loss: 0.06, omega:1142.69, group_sparsity: 0.70, acc1: 0.9248\n",
      "Epoch: 90, loss: 0.06, omega:1142.09, group_sparsity: 0.70, acc1: 0.9234\n",
      "Epoch: 91, loss: 0.05, omega:1141.47, group_sparsity: 0.70, acc1: 0.9241\n",
      "Epoch: 92, loss: 0.05, omega:1140.86, group_sparsity: 0.70, acc1: 0.9252\n",
      "Epoch: 93, loss: 0.05, omega:1140.25, group_sparsity: 0.70, acc1: 0.9256\n",
      "Epoch: 94, loss: 0.05, omega:1139.63, group_sparsity: 0.70, acc1: 0.9258\n",
      "Epoch: 95, loss: 0.05, omega:1139.00, group_sparsity: 0.70, acc1: 0.9255\n",
      "Epoch: 96, loss: 0.05, omega:1138.40, group_sparsity: 0.70, acc1: 0.9252\n",
      "Epoch: 97, loss: 0.04, omega:1137.81, group_sparsity: 0.70, acc1: 0.9245\n",
      "Epoch: 98, loss: 0.04, omega:1137.17, group_sparsity: 0.70, acc1: 0.9258\n",
      "Epoch: 99, loss: 0.04, omega:1136.56, group_sparsity: 0.70, acc1: 0.9247\n",
      "Epoch: 100, loss: 0.04, omega:1135.95, group_sparsity: 0.70, acc1: 0.9237\n",
      "Epoch: 101, loss: 0.04, omega:1135.33, group_sparsity: 0.70, acc1: 0.9248\n",
      "Epoch: 102, loss: 0.04, omega:1134.70, group_sparsity: 0.70, acc1: 0.9257\n",
      "Epoch: 103, loss: 0.04, omega:1134.08, group_sparsity: 0.70, acc1: 0.9238\n",
      "Epoch: 104, loss: 0.04, omega:1133.45, group_sparsity: 0.70, acc1: 0.9238\n",
      "Epoch: 105, loss: 0.04, omega:1132.84, group_sparsity: 0.70, acc1: 0.9228\n",
      "Epoch: 106, loss: 0.04, omega:1132.22, group_sparsity: 0.70, acc1: 0.9245\n",
      "Epoch: 107, loss: 0.04, omega:1131.62, group_sparsity: 0.70, acc1: 0.9246\n",
      "Epoch: 108, loss: 0.03, omega:1130.99, group_sparsity: 0.70, acc1: 0.9257\n",
      "Epoch: 109, loss: 0.03, omega:1130.38, group_sparsity: 0.70, acc1: 0.9255\n",
      "Epoch: 110, loss: 0.03, omega:1129.77, group_sparsity: 0.70, acc1: 0.9245\n",
      "Epoch: 111, loss: 0.03, omega:1129.15, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 112, loss: 0.03, omega:1128.52, group_sparsity: 0.70, acc1: 0.9237\n",
      "Epoch: 113, loss: 0.03, omega:1127.89, group_sparsity: 0.70, acc1: 0.9241\n",
      "Epoch: 114, loss: 0.03, omega:1127.26, group_sparsity: 0.70, acc1: 0.9238\n",
      "Epoch: 115, loss: 0.03, omega:1126.65, group_sparsity: 0.70, acc1: 0.9237\n",
      "Epoch: 116, loss: 0.03, omega:1126.02, group_sparsity: 0.70, acc1: 0.9245\n",
      "Epoch: 117, loss: 0.03, omega:1125.41, group_sparsity: 0.70, acc1: 0.9262\n",
      "Epoch: 118, loss: 0.03, omega:1124.79, group_sparsity: 0.70, acc1: 0.9258\n",
      "Epoch: 119, loss: 0.03, omega:1124.18, group_sparsity: 0.70, acc1: 0.9257\n",
      "Epoch: 120, loss: 0.03, omega:1123.58, group_sparsity: 0.70, acc1: 0.9240\n",
      "Epoch: 121, loss: 0.03, omega:1122.96, group_sparsity: 0.70, acc1: 0.9227\n",
      "Epoch: 122, loss: 0.03, omega:1122.34, group_sparsity: 0.70, acc1: 0.9251\n",
      "Epoch: 123, loss: 0.02, omega:1121.71, group_sparsity: 0.70, acc1: 0.9254\n",
      "Epoch: 124, loss: 0.02, omega:1121.07, group_sparsity: 0.70, acc1: 0.9204\n",
      "Epoch: 125, loss: 0.02, omega:1120.47, group_sparsity: 0.70, acc1: 0.9245\n",
      "Epoch: 126, loss: 0.02, omega:1119.83, group_sparsity: 0.70, acc1: 0.9218\n",
      "Epoch: 127, loss: 0.02, omega:1119.21, group_sparsity: 0.70, acc1: 0.9268\n",
      "Epoch: 128, loss: 0.02, omega:1118.60, group_sparsity: 0.70, acc1: 0.9271\n",
      "Epoch: 129, loss: 0.02, omega:1117.97, group_sparsity: 0.70, acc1: 0.9249\n",
      "Epoch: 130, loss: 0.02, omega:1117.34, group_sparsity: 0.70, acc1: 0.9266\n",
      "Epoch: 131, loss: 0.02, omega:1116.72, group_sparsity: 0.70, acc1: 0.9240\n",
      "Epoch: 132, loss: 0.02, omega:1116.07, group_sparsity: 0.70, acc1: 0.9260\n",
      "Epoch: 133, loss: 0.02, omega:1115.43, group_sparsity: 0.70, acc1: 0.9260\n",
      "Epoch: 134, loss: 0.02, omega:1114.81, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 135, loss: 0.02, omega:1114.18, group_sparsity: 0.70, acc1: 0.9237\n",
      "Epoch: 136, loss: 0.02, omega:1113.55, group_sparsity: 0.70, acc1: 0.9272\n",
      "Epoch: 137, loss: 0.02, omega:1112.93, group_sparsity: 0.70, acc1: 0.9256\n",
      "Epoch: 138, loss: 0.02, omega:1112.29, group_sparsity: 0.70, acc1: 0.9251\n",
      "Epoch: 139, loss: 0.02, omega:1111.66, group_sparsity: 0.70, acc1: 0.9232\n",
      "Epoch: 140, loss: 0.02, omega:1111.01, group_sparsity: 0.70, acc1: 0.9253\n",
      "Epoch: 141, loss: 0.02, omega:1110.35, group_sparsity: 0.70, acc1: 0.9240\n",
      "Epoch: 142, loss: 0.02, omega:1109.72, group_sparsity: 0.70, acc1: 0.9216\n",
      "Epoch: 143, loss: 0.02, omega:1109.08, group_sparsity: 0.70, acc1: 0.9256\n",
      "Epoch: 144, loss: 0.02, omega:1108.46, group_sparsity: 0.70, acc1: 0.9247\n",
      "Epoch: 145, loss: 0.02, omega:1107.83, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 146, loss: 0.02, omega:1107.20, group_sparsity: 0.70, acc1: 0.9270\n",
      "Epoch: 147, loss: 0.02, omega:1106.57, group_sparsity: 0.70, acc1: 0.9251\n",
      "Epoch: 148, loss: 0.02, omega:1105.94, group_sparsity: 0.70, acc1: 0.9226\n",
      "Epoch: 149, loss: 0.01, omega:1105.86, group_sparsity: 0.70, acc1: 0.9243\n",
      "Epoch: 150, loss: 0.01, omega:1105.78, group_sparsity: 0.70, acc1: 0.9247\n",
      "Epoch: 151, loss: 0.01, omega:1105.70, group_sparsity: 0.70, acc1: 0.9256\n",
      "Epoch: 152, loss: 0.01, omega:1105.62, group_sparsity: 0.70, acc1: 0.9252\n",
      "Epoch: 153, loss: 0.01, omega:1105.54, group_sparsity: 0.70, acc1: 0.9259\n",
      "Epoch: 154, loss: 0.01, omega:1105.47, group_sparsity: 0.70, acc1: 0.9249\n",
      "Epoch: 155, loss: 0.01, omega:1105.39, group_sparsity: 0.70, acc1: 0.9259\n",
      "Epoch: 156, loss: 0.01, omega:1105.31, group_sparsity: 0.70, acc1: 0.9265\n",
      "Epoch: 157, loss: 0.01, omega:1105.23, group_sparsity: 0.70, acc1: 0.9272\n",
      "Epoch: 158, loss: 0.01, omega:1105.16, group_sparsity: 0.70, acc1: 0.9268\n",
      "Epoch: 159, loss: 0.01, omega:1105.08, group_sparsity: 0.70, acc1: 0.9272\n",
      "Epoch: 160, loss: 0.01, omega:1105.00, group_sparsity: 0.70, acc1: 0.9270\n",
      "Epoch: 161, loss: 0.01, omega:1104.92, group_sparsity: 0.70, acc1: 0.9263\n",
      "Epoch: 162, loss: 0.01, omega:1104.84, group_sparsity: 0.70, acc1: 0.9266\n",
      "Epoch: 163, loss: 0.01, omega:1104.77, group_sparsity: 0.70, acc1: 0.9274\n",
      "Epoch: 164, loss: 0.01, omega:1104.69, group_sparsity: 0.70, acc1: 0.9271\n",
      "Epoch: 165, loss: 0.01, omega:1104.61, group_sparsity: 0.70, acc1: 0.9271\n",
      "Epoch: 166, loss: 0.01, omega:1104.53, group_sparsity: 0.70, acc1: 0.9261\n",
      "Epoch: 167, loss: 0.01, omega:1104.45, group_sparsity: 0.70, acc1: 0.9269\n",
      "Epoch: 168, loss: 0.01, omega:1104.37, group_sparsity: 0.70, acc1: 0.9261\n",
      "Epoch: 169, loss: 0.01, omega:1104.29, group_sparsity: 0.70, acc1: 0.9280\n",
      "Epoch: 170, loss: 0.01, omega:1104.21, group_sparsity: 0.70, acc1: 0.9262\n",
      "Epoch: 171, loss: 0.01, omega:1104.14, group_sparsity: 0.70, acc1: 0.9276\n",
      "Epoch: 172, loss: 0.01, omega:1104.06, group_sparsity: 0.70, acc1: 0.9273\n",
      "Epoch: 173, loss: 0.01, omega:1103.98, group_sparsity: 0.70, acc1: 0.9271\n",
      "Epoch: 174, loss: 0.01, omega:1103.90, group_sparsity: 0.70, acc1: 0.9260\n",
      "Epoch: 175, loss: 0.01, omega:1103.82, group_sparsity: 0.70, acc1: 0.9280\n",
      "Epoch: 176, loss: 0.01, omega:1103.75, group_sparsity: 0.70, acc1: 0.9281\n",
      "Epoch: 177, loss: 0.01, omega:1103.67, group_sparsity: 0.70, acc1: 0.9280\n",
      "Epoch: 178, loss: 0.01, omega:1103.59, group_sparsity: 0.70, acc1: 0.9278\n",
      "Epoch: 179, loss: 0.01, omega:1103.51, group_sparsity: 0.70, acc1: 0.9284\n",
      "Epoch: 180, loss: 0.01, omega:1103.44, group_sparsity: 0.70, acc1: 0.9266\n",
      "Epoch: 181, loss: 0.01, omega:1103.36, group_sparsity: 0.70, acc1: 0.9268\n",
      "Epoch: 182, loss: 0.01, omega:1103.28, group_sparsity: 0.70, acc1: 0.9281\n",
      "Epoch: 183, loss: 0.01, omega:1103.20, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 184, loss: 0.01, omega:1103.12, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 185, loss: 0.01, omega:1103.04, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 186, loss: 0.01, omega:1102.96, group_sparsity: 0.70, acc1: 0.9253\n",
      "Epoch: 187, loss: 0.01, omega:1102.89, group_sparsity: 0.70, acc1: 0.9268\n",
      "Epoch: 188, loss: 0.01, omega:1102.81, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 189, loss: 0.01, omega:1102.73, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 190, loss: 0.01, omega:1102.66, group_sparsity: 0.70, acc1: 0.9260\n",
      "Epoch: 191, loss: 0.01, omega:1102.58, group_sparsity: 0.70, acc1: 0.9282\n",
      "Epoch: 192, loss: 0.01, omega:1102.50, group_sparsity: 0.70, acc1: 0.9284\n",
      "Epoch: 193, loss: 0.01, omega:1102.42, group_sparsity: 0.70, acc1: 0.9278\n",
      "Epoch: 194, loss: 0.01, omega:1102.34, group_sparsity: 0.70, acc1: 0.9270\n",
      "Epoch: 195, loss: 0.01, omega:1102.26, group_sparsity: 0.70, acc1: 0.9277\n",
      "Epoch: 196, loss: 0.01, omega:1102.19, group_sparsity: 0.70, acc1: 0.9261\n",
      "Epoch: 197, loss: 0.01, omega:1102.11, group_sparsity: 0.70, acc1: 0.9269\n",
      "Epoch: 198, loss: 0.01, omega:1102.03, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 199, loss: 0.01, omega:1101.95, group_sparsity: 0.70, acc1: 0.9263\n",
      "Epoch: 200, loss: 0.01, omega:1101.87, group_sparsity: 0.70, acc1: 0.9280\n",
      "Epoch: 201, loss: 0.01, omega:1101.80, group_sparsity: 0.70, acc1: 0.9277\n",
      "Epoch: 202, loss: 0.01, omega:1101.72, group_sparsity: 0.70, acc1: 0.9274\n",
      "Epoch: 203, loss: 0.01, omega:1101.64, group_sparsity: 0.70, acc1: 0.9265\n",
      "Epoch: 204, loss: 0.01, omega:1101.56, group_sparsity: 0.70, acc1: 0.9273\n",
      "Epoch: 205, loss: 0.01, omega:1101.48, group_sparsity: 0.70, acc1: 0.9279\n",
      "Epoch: 206, loss: 0.01, omega:1101.41, group_sparsity: 0.70, acc1: 0.9277\n",
      "Epoch: 207, loss: 0.01, omega:1101.33, group_sparsity: 0.70, acc1: 0.9270\n",
      "Epoch: 208, loss: 0.01, omega:1101.25, group_sparsity: 0.70, acc1: 0.9259\n",
      "Epoch: 209, loss: 0.01, omega:1101.17, group_sparsity: 0.70, acc1: 0.9259\n",
      "Epoch: 210, loss: 0.01, omega:1101.09, group_sparsity: 0.70, acc1: 0.9268\n",
      "Epoch: 211, loss: 0.01, omega:1101.02, group_sparsity: 0.70, acc1: 0.9275\n",
      "Epoch: 212, loss: 0.01, omega:1100.94, group_sparsity: 0.70, acc1: 0.9283\n",
      "Epoch: 213, loss: 0.01, omega:1100.86, group_sparsity: 0.70, acc1: 0.9278\n",
      "Epoch: 214, loss: 0.01, omega:1100.78, group_sparsity: 0.70, acc1: 0.9276\n",
      "Epoch: 215, loss: 0.01, omega:1100.70, group_sparsity: 0.70, acc1: 0.9271\n",
      "Epoch: 216, loss: 0.01, omega:1100.62, group_sparsity: 0.70, acc1: 0.9275\n",
      "Epoch: 217, loss: 0.01, omega:1100.55, group_sparsity: 0.70, acc1: 0.9279\n",
      "Epoch: 218, loss: 0.01, omega:1100.47, group_sparsity: 0.70, acc1: 0.9262\n",
      "Epoch: 219, loss: 0.01, omega:1100.39, group_sparsity: 0.70, acc1: 0.9265\n",
      "Epoch: 220, loss: 0.01, omega:1100.31, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 221, loss: 0.01, omega:1100.23, group_sparsity: 0.70, acc1: 0.9280\n",
      "Epoch: 222, loss: 0.01, omega:1100.15, group_sparsity: 0.70, acc1: 0.9255\n",
      "Epoch: 223, loss: 0.01, omega:1100.07, group_sparsity: 0.70, acc1: 0.9269\n",
      "Epoch: 224, loss: 0.01, omega:1100.07, group_sparsity: 0.70, acc1: 0.9272\n",
      "Epoch: 225, loss: 0.01, omega:1100.06, group_sparsity: 0.70, acc1: 0.9265\n",
      "Epoch: 226, loss: 0.01, omega:1100.05, group_sparsity: 0.70, acc1: 0.9281\n",
      "Epoch: 227, loss: 0.01, omega:1100.05, group_sparsity: 0.70, acc1: 0.9264\n",
      "Epoch: 228, loss: 0.01, omega:1100.04, group_sparsity: 0.70, acc1: 0.9270\n",
      "Epoch: 229, loss: 0.01, omega:1100.03, group_sparsity: 0.70, acc1: 0.9260\n",
      "Epoch: 230, loss: 0.01, omega:1100.03, group_sparsity: 0.70, acc1: 0.9266\n",
      "Epoch: 231, loss: 0.01, omega:1100.02, group_sparsity: 0.70, acc1: 0.9261\n",
      "Epoch: 232, loss: 0.01, omega:1100.02, group_sparsity: 0.70, acc1: 0.9259\n",
      "Epoch: 233, loss: 0.01, omega:1100.01, group_sparsity: 0.70, acc1: 0.9282\n",
      "Epoch: 234, loss: 0.01, omega:1100.00, group_sparsity: 0.70, acc1: 0.9277\n",
      "Epoch: 235, loss: 0.01, omega:1100.00, group_sparsity: 0.70, acc1: 0.9268\n",
      "Epoch: 236, loss: 0.01, omega:1099.99, group_sparsity: 0.70, acc1: 0.9283\n",
      "Epoch: 237, loss: 0.01, omega:1099.98, group_sparsity: 0.70, acc1: 0.9273\n",
      "Epoch: 238, loss: 0.01, omega:1099.98, group_sparsity: 0.70, acc1: 0.9271\n",
      "Epoch: 239, loss: 0.01, omega:1099.97, group_sparsity: 0.70, acc1: 0.9282\n",
      "Epoch: 240, loss: 0.01, omega:1099.96, group_sparsity: 0.70, acc1: 0.9269\n",
      "Epoch: 241, loss: 0.01, omega:1099.96, group_sparsity: 0.70, acc1: 0.9275\n",
      "Epoch: 242, loss: 0.01, omega:1099.95, group_sparsity: 0.70, acc1: 0.9269\n",
      "Epoch: 243, loss: 0.01, omega:1099.94, group_sparsity: 0.70, acc1: 0.9272\n",
      "Epoch: 244, loss: 0.01, omega:1099.94, group_sparsity: 0.70, acc1: 0.9270\n",
      "Epoch: 245, loss: 0.01, omega:1099.93, group_sparsity: 0.70, acc1: 0.9281\n",
      "Epoch: 246, loss: 0.01, omega:1099.93, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 247, loss: 0.01, omega:1099.92, group_sparsity: 0.70, acc1: 0.9272\n",
      "Epoch: 248, loss: 0.01, omega:1099.91, group_sparsity: 0.70, acc1: 0.9272\n",
      "Epoch: 249, loss: 0.01, omega:1099.91, group_sparsity: 0.70, acc1: 0.9270\n",
      "Epoch: 250, loss: 0.01, omega:1099.90, group_sparsity: 0.70, acc1: 0.9266\n",
      "Epoch: 251, loss: 0.01, omega:1099.89, group_sparsity: 0.70, acc1: 0.9263\n",
      "Epoch: 252, loss: 0.01, omega:1099.89, group_sparsity: 0.70, acc1: 0.9265\n",
      "Epoch: 253, loss: 0.01, omega:1099.88, group_sparsity: 0.70, acc1: 0.9278\n",
      "Epoch: 254, loss: 0.01, omega:1099.87, group_sparsity: 0.70, acc1: 0.9269\n",
      "Epoch: 255, loss: 0.01, omega:1099.87, group_sparsity: 0.70, acc1: 0.9270\n",
      "Epoch: 256, loss: 0.01, omega:1099.86, group_sparsity: 0.70, acc1: 0.9273\n",
      "Epoch: 257, loss: 0.01, omega:1099.85, group_sparsity: 0.70, acc1: 0.9266\n",
      "Epoch: 258, loss: 0.01, omega:1099.85, group_sparsity: 0.70, acc1: 0.9277\n",
      "Epoch: 259, loss: 0.01, omega:1099.84, group_sparsity: 0.70, acc1: 0.9279\n",
      "Epoch: 260, loss: 0.01, omega:1099.84, group_sparsity: 0.70, acc1: 0.9281\n",
      "Epoch: 261, loss: 0.01, omega:1099.83, group_sparsity: 0.70, acc1: 0.9279\n",
      "Epoch: 262, loss: 0.01, omega:1099.82, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 263, loss: 0.01, omega:1099.82, group_sparsity: 0.70, acc1: 0.9284\n",
      "Epoch: 264, loss: 0.01, omega:1099.81, group_sparsity: 0.70, acc1: 0.9273\n",
      "Epoch: 265, loss: 0.01, omega:1099.80, group_sparsity: 0.70, acc1: 0.9276\n",
      "Epoch: 266, loss: 0.01, omega:1099.80, group_sparsity: 0.70, acc1: 0.9264\n",
      "Epoch: 267, loss: 0.01, omega:1099.79, group_sparsity: 0.70, acc1: 0.9276\n",
      "Epoch: 268, loss: 0.01, omega:1099.78, group_sparsity: 0.70, acc1: 0.9268\n",
      "Epoch: 269, loss: 0.01, omega:1099.78, group_sparsity: 0.70, acc1: 0.9269\n",
      "Epoch: 270, loss: 0.01, omega:1099.77, group_sparsity: 0.70, acc1: 0.9264\n",
      "Epoch: 271, loss: 0.01, omega:1099.77, group_sparsity: 0.70, acc1: 0.9288\n",
      "Epoch: 272, loss: 0.01, omega:1099.76, group_sparsity: 0.70, acc1: 0.9286\n",
      "Epoch: 273, loss: 0.01, omega:1099.75, group_sparsity: 0.70, acc1: 0.9282\n",
      "Epoch: 274, loss: 0.01, omega:1099.75, group_sparsity: 0.70, acc1: 0.9277\n",
      "Epoch: 275, loss: 0.01, omega:1099.74, group_sparsity: 0.70, acc1: 0.9254\n",
      "Epoch: 276, loss: 0.01, omega:1099.73, group_sparsity: 0.70, acc1: 0.9279\n",
      "Epoch: 277, loss: 0.01, omega:1099.73, group_sparsity: 0.70, acc1: 0.9264\n",
      "Epoch: 278, loss: 0.01, omega:1099.72, group_sparsity: 0.70, acc1: 0.9277\n",
      "Epoch: 279, loss: 0.01, omega:1099.71, group_sparsity: 0.70, acc1: 0.9280\n",
      "Epoch: 280, loss: 0.01, omega:1099.71, group_sparsity: 0.70, acc1: 0.9276\n",
      "Epoch: 281, loss: 0.01, omega:1099.70, group_sparsity: 0.70, acc1: 0.9271\n",
      "Epoch: 282, loss: 0.01, omega:1099.69, group_sparsity: 0.70, acc1: 0.9279\n",
      "Epoch: 283, loss: 0.01, omega:1099.69, group_sparsity: 0.70, acc1: 0.9272\n",
      "Epoch: 284, loss: 0.01, omega:1099.68, group_sparsity: 0.70, acc1: 0.9275\n",
      "Epoch: 285, loss: 0.01, omega:1099.68, group_sparsity: 0.70, acc1: 0.9274\n",
      "Epoch: 286, loss: 0.01, omega:1099.67, group_sparsity: 0.70, acc1: 0.9286\n",
      "Epoch: 287, loss: 0.01, omega:1099.66, group_sparsity: 0.70, acc1: 0.9276\n",
      "Epoch: 288, loss: 0.01, omega:1099.66, group_sparsity: 0.70, acc1: 0.9263\n",
      "Epoch: 289, loss: 0.01, omega:1099.65, group_sparsity: 0.70, acc1: 0.9281\n",
      "Epoch: 290, loss: 0.01, omega:1099.64, group_sparsity: 0.70, acc1: 0.9275\n",
      "Epoch: 291, loss: 0.01, omega:1099.64, group_sparsity: 0.70, acc1: 0.9269\n",
      "Epoch: 292, loss: 0.01, omega:1099.63, group_sparsity: 0.70, acc1: 0.9285\n",
      "Epoch: 293, loss: 0.01, omega:1099.62, group_sparsity: 0.70, acc1: 0.9277\n",
      "Epoch: 294, loss: 0.01, omega:1099.62, group_sparsity: 0.70, acc1: 0.9280\n",
      "Epoch: 295, loss: 0.01, omega:1099.61, group_sparsity: 0.70, acc1: 0.9265\n",
      "Epoch: 296, loss: 0.01, omega:1099.60, group_sparsity: 0.70, acc1: 0.9276\n",
      "Epoch: 297, loss: 0.01, omega:1099.60, group_sparsity: 0.70, acc1: 0.9267\n",
      "Epoch: 298, loss: 0.01, omega:1099.59, group_sparsity: 0.70, acc1: 0.9278\n",
      "Epoch: 299, loss: 0.01, omega:1099.59, group_sparsity: 0.70, acc1: 0.9286\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import check_accuracy\n",
    "\n",
    "max_epoch = 300\n",
    "model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# Every 75 epochs, decay lr by 10.0\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=75, gamma=0.1) \n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    f_avg_val = 0.0\n",
    "    model.train()\n",
    "    lr_scheduler.step()\n",
    "    for X, y in trainloader:\n",
    "        X = X.cuda()\n",
    "        y = y.cuda()\n",
    "        y_pred = model.forward(X)\n",
    "        f = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        f.backward()\n",
    "        f_avg_val += f\n",
    "        optimizer.step()\n",
    "    group_sparsity, omega = optimizer.compute_group_sparsity_omega()\n",
    "    accuracy1, accuracy5 = check_accuracy(model, testloader)\n",
    "    f_avg_val = f_avg_val.cpu().item() / len(trainloader)\n",
    "    print(\"Epoch: {ep}, loss: {f:.2f}, omega:{om:.2f}, group_sparsity: {gs:.2f}, acc1: {acc:.4f}\".format(ep=epoch, f=f_avg_val, om=omega, gs=group_sparsity, acc=accuracy1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. Get compressed model in ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A ResNet_compressed.onnx will be generated. \n",
    "oto.compress()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Compute FLOPs and number of parameters before and after OTO training\n",
    "\n",
    "The compressed ResNet18 only uses 9% of the parameters about 20% of the FLOPs compared to the full model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full FLOPs (M): 555.42. Compressed FLOPs (M): 112.52. Reduction Ratio: 0.7974\n",
      "Full # Params: 11173962. Compressed # Params: 981808. Reduction Ratio: 0.9121\n"
     ]
    }
   ],
   "source": [
    "full_flops = oto.compute_flops()\n",
    "compressed_flops = oto.compute_flops(compressed=True)\n",
    "full_num_params = oto.compute_num_params()\n",
    "compressed_num_params = oto.compute_num_params(compressed=True)\n",
    "\n",
    "print(\"Full FLOPs (M): {f_flops:.2f}. Compressed FLOPs (M): {c_flops:.2f}. Reduction Ratio: {f_ratio:.4f}\"\\\n",
    "      .format(f_flops=full_flops, c_flops=compressed_flops, f_ratio=1 - compressed_flops/full_flops))\n",
    "print(\"Full # Params: {f_params}. Compressed # Params: {c_params}. Reduction Ratio: {f_ratio:.4f}\"\\\n",
    "      .format(f_params=full_num_params, c_params=compressed_num_params, f_ratio=1 - compressed_num_params/full_num_params))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Check the compressed model accuracy\n",
    "\n",
    "#### Both full and compressed model should return the exact same accuracy.\n",
    "\n",
    "Compared to the baseline of full ResNet18 on CIFAR10, the compressed ResNet18 only regresses 0.1% top-1 accuracy, but significantly reduces FLOPs and params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full model: Acc 1: 0.9286, Acc 5: 0.9974\n",
      "Compressed model: Acc 1: 0.9286, Acc 5: 0.9974\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import check_accuracy_onnx\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "acc1_full, acc5_full = check_accuracy(model, testloader)\n",
    "print(\"Full model: Acc 1: {acc1}, Acc 5: {acc5}\".format(acc1=acc1_full, acc5=acc5_full))\n",
    "\n",
    "acc1_compressed, acc5_compressed = check_accuracy_onnx(oto.compressed_model_path, testloader)\n",
    "print(\"Compressed model: Acc 1: {acc1}, Acc 5: {acc5}\".format(acc1=acc1_compressed, acc5=acc5_compressed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
